1.Purpose of Using Spark with Scala

  using Spark with Scala because it offers high-performance distributed data processing using in-memory computation.
 Since Spark is written in Scala, we will get better performance, full API coverage, and tighter integration compared to other languages
 like Python. It helps us handle massive volumes of data.

2. Why Use Spark + Scala Instead of Just SQL?
 We use Spark with Scala instead of plain SQL because SQL can't scale to big data.
 Spark is designed to handle large-scale, distributed data processing efficiently.
 Scala gives us the flexibility of a full programming language, allowing custom logic,
 reusable ETL pipelines, and better performance due to in-memory computing — something traditional SQL cannot offer

3.Why Use Scala for Spark Instead of Other Languages?
 We prefer Scala for Spark because it's Spark's native language, offering the best performance and full API access. 
 Scala supports functional programming, has strong compile-time type safety, and is highly expressive.
 This makes it perfect for writing scalable, reliable, and efficient data pipelines
 in production — something Python or Java can't offer to the same degree.

4. Why We Use Spark + Scala in Big Data Testing Projects

 Schema and Data Quality Testing
   With Spark + Scala, testers can write:

   Schema validation
   Null checks
   Type checks
   Range checks
    Duplicate checks
   df.filter($"age".isNull).count() // Null check
   df.dropDuplicates().count()      // Deduplication check

.Join, Filter, and Transformation Testing
   You can easily test:
   Joins between source and lookup tables
   Filtering logic
   Aggregation correctness
   Transformation rules (e.g., column derivation)
   “We validate transformation rules using Spark's functional operations like filter, select, and groupBy.”

  In our project, we use Spark with Scala for Big Data Testing because it allows us to efficiently validate large datasets, 
perform transformation and schema checks, and automate test cases for ETL pipelines. 
Since the development is also done in Spark, using Scala gives us tight integration, better performance, and consistency.

  Testing on Cloud (e.g., AWS S3, EMR, Glue)
Spark with Scala runs on EMR, reads from S3, and integrates with Athena/Glue.
    
    Read input files from S3
    Validate written output
    Reconcile source and target
    “We use Spark Scala scripts to validate data flowing from S3 through Glue and into Redshift/Athena.”
 
5 Why is Spark used in your project instead of traditional SQL or ETL tools?
Expected Answer:

We process large volumes of data, and traditional tools like SQL or Informatica weren’t scalable enough.
 Spark allows us to perform distributed data validation and ETL logic testing efficiently using in-memory computing.

6 What is a DataFrame in Spark?

A DataFrame is a distributed collection of data organized into named columns,
 similar to a table in a relational database. In testing, we use it to apply filters, joins, and aggregations during validation.

7. How do you perform schema validation in Spark?

We use df.printSchema() and df.schema.fields to compare the actual schema against the expected.
 We also assert data types and nullability of columns.

8. How do you check for duplicates in a dataset using Scala?


     val totalCount = df.count()
     val uniqueCount = df.dropDuplicates().count()

 What would you do if duplicates exist?
    Log and report them, or fail the test if data quality is critical.

9. How do you compare two DataFrames in Spark?

    val diff1 = df1.except(df2)
    val diff2 = df2.except(df1)
    val isEqual = diff1.count() == 0 && diff2.count() == 0
     Used to check reconciliation between source and target.

10. How do you validate null values in Spark?

    df.filter($"columnName".isNull).count()
We count null values for each required column and raise defects if values are missing.

11. What is the use of dropDuplicates() in Spark?

    Removes duplicate rows based on all or selected columns. We use it to check data uniqueness as part of data quality testing.

12. How do you perform filtering in Spark using Scala?

    val filteredDF = df.filter($"age" > 30 && $"status" === "active")
     Used to validate filtered outputs or test transformation conditions.

13. How do you test a transformation logic in Spark?

    By applying the same transformation manually in test code and comparing expected vs actual output using DataFrame assertions.

14. How do you validate partitioned output in S3/HDFS?

      We check folder structure (e.g., year=2025/month=06) and 
validate file count or records in each partition directory using spark.read.option(...).load(...).

15. What is lazy evaluation in Spark, and why is it important?

   Spark doesn’t execute operations until an action (count, show, write) is called.
 This improves performance and allows optimization. As testers, we know validations run only when actions are triggered.

16. How do you log errors or mismatches during testing in Spark Scala?

      mismatches.write.mode("overwrite").csv("error_output/")
      To store invalid rows for debugging.

17. Difference between repartition() and coalesce()?

    Function	Use Case
   repartition	Increases or redistributes partitions
   coalesce	Reduces number of partitions (efficient)

We use them during performance testing or when writing optimized output.

18. what types of testing  performed on Spark-based applications?


    Data validation (source vs target)
    Schema validation
    Null and duplicate checks
    Transformation logic testing
    Partition and output testing
    Performance/SLA testing

19 . How do you validate whether the Spark job wrote the correct output files (CSV/Parquet) in S3 or HDFS?
By checking:

Folder path (e.g., s3://bucket/year=2024/month=06)
Record count in the output
Schema using spark.read.parquet().printSchema()
File format and size

20. How do you test if the transformation logic in the Spark pipeline is correct?

By replicating the transformation manually in test code using:

   val actualDF = spark.read.csv("output.csv")
   val expectedDF = referenceData // manually created or derived
   assert(actualDF.except(expectedDF).isEmpty)

21. How do you compare two DataFrames in Spark as part of validation testing?

val diff1 = df1.except(df2)
val diff2 = df2.except(df1)
val isEqual = diff1.isEmpty && diff2.isEmpty

22. What is Spark lazy evaluation? How is it important in testing?

Spark doesn't execute transformations (like select, filter) until an action (show, count, write) is triggered.
As a tester, you must use .count() or .collect() to execute validations.

23. How do you validate large file processing?

    Record count check (source vs target)
    Summary stats (min, max, avg values)
    Row-level sampling
    Reconciliation reports



